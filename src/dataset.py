# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import copy
import torch
import numpy as np
import pandas as pd

from qlib.data.dataset import DatasetH
from qlib.data import D

device = "cuda:0" if torch.cuda.is_available() else "cpu"


def _to_tensor(x):
    if not isinstance(x, torch.Tensor):
        return torch.tensor(x, dtype=torch.float, device=device)
    return x


def _create_ts_slices(index, seq_len):
    """
    create time series slices from pandas index

    Args:
        index (pd.MultiIndex): pandas multiindex with <instrument, datetime> order
        seq_len (int): sequence length
    """
    # assert index.is_lexsorted(), "index should be sorted"

    # number of dates for each code
    sample_count_by_codes = pd.Series(0, index=index).groupby(level=0).size().values

    # start_index for each code
    start_index_of_codes = np.roll(np.cumsum(sample_count_by_codes), 1)
    if len(start_index_of_codes) == 0:
        return []
    start_index_of_codes[0] = 0

    # all the [start, stop) indices of features
    # features btw [start, stop) are used to predict the `stop - 1` label
    slices = []
    for cur_loc, cur_cnt in zip(start_index_of_codes, sample_count_by_codes):
        for stop in range(1, cur_cnt + 1):
            end = cur_loc + stop
            start = max(end - seq_len, 0)
            slices.append(slice(start, end))
    slices = np.array(slices)

    return slices



def _get_date_parse_fn(target):
    # This forces everything (strings, dates, timestamps) into a pandas Timestamp
    # ensuring it matches the 'pandas._libs.tslibs.timestamps.Timestamp' in your file.
    return lambda x: pd.Timestamp(x)


class UnifiedTSDatasetH(DatasetH):
    """Memory Augmented Time Series Dataset

    Args:
        handler (DataHandler): data handler
        segments (dict): data split segments
        seq_len (int): time series sequence length
        horizon (int): label horizon (to mask historical loss for TRA)
        batch_size (int): batch size (<0 means daily batch)
        shuffle (bool): whether shuffle data
        pin_memory (bool): whether pin data to gpu memory
        drop_last (bool): whether drop last batch < batch_size
    """

    def __init__(
        self,
        handler,
        segments,
        seq_len=20,
        horizon=10,
        batch_size=-1,
        shuffle=True,
        pin_memory=False,
        drop_last=False,
        account=1000000,
        **kwargs,
    ):

        assert horizon > 0, "please specify `horizon` to avoid data leakage"

        self.seq_len = seq_len
        self.horizon = horizon
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.pin_memory = pin_memory
        self.params = (batch_size, drop_last, shuffle)  # for train/eval switch

        self.account = account
        self.freq = "day"
        self.benchmark = "SH000300"

        self.executor_config = {
            "class": "SimulatorExecutor",
            "module_path": "qlib.backtest.executor",
            "kwargs": {
                "time_per_step": "day",
                "generate_portfolio_metrics": True,
            },
        }
        self.backtest_config = {
            "start_time": segments['test'][0],
            "end_time": segments['test'][1],
            "account": self.account,
            "benchmark": self.benchmark, # default
            "exchange_kwargs": {
                "freq": self.freq,
                "limit_threshold": 0.095,
                "deal_price": "close",
                "open_cost": 0.0005,
                "close_cost": 0.0015,
                "min_cost": 5,
            },
        }

        super().__init__(handler, segments, **kwargs)

    def setup_data(self, handler_kwargs: dict = None, **kwargs):

        super().setup_data()

        # change index to <code, date>
        # NOTE: we will use inplace sort to reduce memory use
        df = self.handler._data
        # df.index = df.index.swaplevel()
        # df.sort_index(inplace=True)
        fea = [col for col in df.columns if col not in ['label', 'source']]

        unique_sources = df['source'].unique()
        df_ = pd.DataFrame()
        for source_value in unique_sources:
            df_source = df[df['source'] == source_value]
            
            # df_source = df_source.set_index(['instrument', 'datetime'])
            df_source.index = df_source.index.swaplevel()
            df_source.sort_index(inplace=True)
            # df_ = df_.append(df_source)
            df_ = pd.concat([df_, df_source])
        df = df_
        self._data = df[fea].squeeze().astype("float32")
        self._source = df['source'].squeeze().astype("int32")
        self._label = df[['label']].squeeze().astype("float32")
        self._index = df.index

        # add memory to feature
        self._data = np.c_[self._data, np.zeros((len(self._data), 1), dtype=np.float32)]

        # padding tensor
        self.zeros = np.zeros((self.seq_len, self._data.shape[1]), dtype=np.float32)

        # pin memory
        if self.pin_memory:
            self._data = _to_tensor(self._data)
            self._label = _to_tensor(self._label)
            self.zeros = _to_tensor(self.zeros)

        # # create batch slices
        # self.batch_slices = _create_ts_slices(self._index, self.seq_len)
        sources = self._source.values
        unique_sources = np.unique(sources)
        self.batch_slices = []
        current_pos = 0

        for source_id in unique_sources:

            mask = (sources[current_pos:] == source_id)
            if not np.any(mask):
                continue
            end_pos = current_pos + np.argmax(~mask) if not np.all(mask) else len(sources)

            source_index = self._index[current_pos:end_pos]
            if len(source_index) == 0:
                continue
            source_slices = _create_ts_slices(source_index, self.seq_len)

            for slc in source_slices:
                global_slice = slice(slc.start + current_pos, slc.stop + current_pos)
                self.batch_slices.append(global_slice)
            current_pos = end_pos

        self.batch_slices = np.array(self.batch_slices)

        index = [slc.stop - 1 for slc in self.batch_slices]
        act_index = self.restore_index(index)
        daily_slices = {date: [] for date in sorted(act_index.unique(level=1))}
        for i, (code, date) in enumerate(act_index):
            daily_slices[date].append(self.batch_slices[i])
        self.daily_slices = list(daily_slices.values())





    def _prepare_seg(self, slc, **kwargs):
        if isinstance(slc, slice):
            start, stop = slc.start, slc.stop
        elif isinstance(slc, (list, tuple)):
            start, stop = slc
        else:
            raise NotImplementedError(f"This type of input is not supported")
            
        start_date = pd.Timestamp(start)
        end_date = pd.Timestamp(stop)
        
        obj = copy.copy(self) 
        obj._data = self._data
        obj._label = self._label
        obj._index = self._index
        
        # Helper to find which index level is the datetime
        # This prevents the DateParseError by finding the actual date values
        dt_level = self._index.names.index('datetime')

        new_batch_slices = []
        for batch_slc in self.batch_slices:
            # We grab the date from the correct level (0 or 1)
            raw_date = self._index[batch_slc.stop - 1][dt_level]
            date = pd.Timestamp(raw_date)
            if start_date <= date <= end_date:
                new_batch_slices.append(batch_slc)
        
        if len(new_batch_slices) == 0:
            print(f"⚠️ Warning: No data found for segment {start_date} to {end_date}")
            obj.batch_slices = np.array([]) 
        else:
            obj.batch_slices = np.array(new_batch_slices)

        new_daily_slices = []
        for daily_slc in self.daily_slices:
            raw_date = self._index[daily_slc[0].stop - 1][dt_level]
            date = pd.Timestamp(raw_date)
            if start_date <= date <= end_date:
                new_daily_slices.append(daily_slc)
        
        obj.daily_slices = new_daily_slices
        return obj

    def restore_index(self, index):
        if isinstance(index, torch.Tensor):
            index = index.cpu().numpy()
        return self._index[index]

    def assign_data(self, index, vals):
        if isinstance(self._data, torch.Tensor):
            vals = _to_tensor(vals)
        elif isinstance(vals, torch.Tensor):
            vals = vals.detach().cpu().numpy()
            index = index.detach().cpu().numpy()
        self._data[index, -1. :] = vals

    def clear_memory(self):
        self._data[:, -1 :] = 0

    # TODO: better train/eval mode design
    def train(self):
        """enable traning mode"""
        self.batch_size, self.drop_last, self.shuffle = self.params

    def eval(self):
        """enable evaluation mode"""
        self.batch_size = self.batch_size
        self.drop_last = False
        self.shuffle = False

    def _get_slices(self):
        if self.batch_size < 0:
            slices = self.daily_slices.copy()
            batch_size = -1 * self.batch_size
        else:
            slices = self.batch_slices.copy()
            batch_size = self.batch_size
        return slices, batch_size

    def __len__(self):
        slices, batch_size = self._get_slices()
        if self.drop_last:
            return len(slices) // batch_size
        return (len(slices) + batch_size - 1) // batch_size

    def __iter__(self):
        slices, batch_size = self._get_slices()
        if self.shuffle:
            np.random.shuffle(slices)

        for i in range(len(slices))[::batch_size]:
            if self.drop_last and i + batch_size > len(slices):
                break
            # get slices for this batch
            slices_subset = slices[i : i + batch_size]
            if self.batch_size < 0:
                slices_subset = np.concatenate(slices_subset)
            # collect data
            data = []
            label = []
            index = []
            for slc in slices_subset:
                _data = self._data[slc].clone() if self.pin_memory else self._data[slc].copy()
                if len(_data) != self.seq_len:
                    if self.pin_memory:
                        _data = torch.cat([self.zeros[: self.seq_len - len(_data)], _data], axis=0)
                    else:
                        _data = np.concatenate([self.zeros[: self.seq_len - len(_data)], _data], axis=0)
                _data[-self.horizon :, -1 :] = 0
                data.append(_data)
                label.append(self._label[slc.stop - 1])
                index.append(slc.stop - 1)


            # concate
            index = torch.tensor(index, device=device)
            if isinstance(data[0], torch.Tensor):
                data = torch.stack(data)
                label = torch.stack(label)
            else:
                data = _to_tensor(np.stack(data))
                label = _to_tensor(np.stack(label))
            # yield -> generator

            yield {"data": data, "label": label, "index": index}